# [Title]

## Introduction

Dans notre vie d'ingénieurs, l'usage des LLMs est devenu de plus en plus incontournable.
Que ce soit de l'aide rédactionnelle, pour faire de la rétro-ingénierie, ou même du développement, c'est un outil 
game-changer dont l'adoption dans la vie quotidienne ne fait plus aucun doute. [parler du fait que cela nous
augmente au même titre que des précédentes révolutions technologiques. donner des exemples]
[Insérer graphe d'adoption de chatgpt et autres LLMs]

Cependant, cela pose un problème sur la souveraineté et de la maîtrise que nous avons sur ces outils: où sont
enregistrées les données que l'on envoie ? A quelles fins sont-elles utilisées? Il ne fait aucun doute pour moi
que pour beaucoup d'utilisateurs de ChatGpt, l'application les connait souvent mieux que les gens qui les entoure.
Et même au delà de ces considérations, nous créons une dépendance, par notre usage quotidien de ces outils, aux plateformes
qui gèrent ces outils. Qu'est-ce qui les empêchera à l'avenir, de multiplier leur prix par 2? Par 3? Ce sont
des phénomènes loins de la fiction, qui arrivent d'ailleurs dans l'industrie du Cloud. Nombre de grands comptes,
ayant migré vers des Cloud Provider (Azure, GCP, AWS...) se retrouvent écrasés par des tarifs sur lesquels ils
perdent peu à peu la main.

Quelles conclusions en tirer? De mon point de vue, il ne faut pas non plus tomber dans la diabolisation de ces outils
et complètement les rejeter. Ils restent de formidables atout quand ils sont bien utilisés. Mais c'est aussi
une opportunité selon moi d'ouvrir le capot de ces services, comprendre comment ils fonctionnent pour mieux les
utiliser, et surtout être capable de s'en défaire si besoin. [Faire une comparaison sur une machine qu'on
utilise tous les jours, le lave-vaisselle par exemple?)]

C'est pour ces raisons que j'ai souhaité me lancer un défi : serait-il possible, en 48h, de déployer en local une solution
fiable en local ?

(TL;DR : Oui c'était possible. Mais il y a un mais...)

Suivez-moi tout au long de cette expérimentation de 48H.

## Chapitre 0. La veille du jour 1

Je sais d'expérience qu'un élément critique de la réussite de ce prototype dépendra pour beaucoup de la 
qualité de son cadrage : avant même de débuter, il faut que je sache où je vais, et surtout, où je ne vais pas.
Je veux aussi en profiter pour mettre en pratique des idées d'un workflow autour des LLMs qui me permettrait
de produire plus vite et mieux, et surtout : je ne veux pas de boîte noire. Je veux apprendre et comprendre tout.

Je décide donc d'écrire un manifeste qui contiendra tout le plan de mon projet : la liste des différentes étapes,
une estimation de charge pour chacune d'entres-elle, et surtout à chaque étape : quels frameworks et librairies utiliser.

Ce manifeste sera un document clef pour piloter le projet. Il m'empêchera de dériver (ce qui n'est pas aisé quand on
découvre de nouvelles techniques, on veut tout savoir, tout de suite, trop vite.) et sera d'une grande aide lorsque, à
plusieurs reprises, je ferais appel à un LLM pour des tâches précises. Cela garantira que je n'oublierais rien et me 
fera gagner du temps, et de la pertinence.

Lors de ces recherches nocturnes, tout semble converger vers un concept. Le RAG. "Retrieval Augmented Generation".
C'est un système qui permet à un LLM de ne pas se contenter de générer une réponse en se basant uniquement sur ses connaissances
de base, mais de s'appuyer sur des sources externes, non connues lors de l'entraînement. 
Cela permet au modèle d'être plus pertinent. 
* des informations fraîches. Pas besoin de réentraîner le modèle chaque jour sur les dernières données
* Réduire les hallucinations. Le corpus fait office de "source de vérité" ce qui réduit énormément les approximations.
[Insérer un graphe minimaliste d'un RAG (LLM qui pioche dans des sources)]
C'est d'ailleurs la technique utilisée par ChatGPT, Gemini, Claude... Pour récupérer des informations sur internet. Cette
technique est assez récente car elle est née dans le milieu de la recherche en 2020 et a été progressivement adoptée 
par l'industrie à partir de 2023.

C'était donc décidé: Il fallait que je mette en place ce système. Je complète le manifeste du projet, dont l'objectif
devient le suivant:

48 heures pour développer un système RAG capable de consulter des sources scientifiques avant de répondre à un prompt.
Avec une interface graphique minimaliste.

## Jour 1 - Partie 1 - Préparatifs

La première étape consiste à se créer un corpus, une bibliographie. Dans mon cas, je décide de récupérer des documents
"fondateurs" dans le monde du Machine Learning.
[mettre les 10 documents]
Ces documents représentent 233 pages d'articles scientifiques et 750 pages pour le livre d'Hastie. Cela constituera un
premier corpus suffisant pour faire des tests.

Ensuite, plus qu'à se retrousser les manches et commencer.

J'installe Ollama, outil par lequel il est possible de télécharger des llms locaux et de faire tourner un serveur
qui leur transmettra nos demandes de chat. Je prends le modèle Llama3, la version avec 8 milliards de paramètres.
Je le lance, je lui pose une question. Mon GPU s'agite, et, après quelques secondes : ma réponse est là! Mais
pour le moment, rien de bien impressionnant.

Afin de tester l'api Python, j'écris un script minimaliste. Jusque là pas d'embûches...

[Montrer un screen avec le script et le résultat]

# Jour 1 - Partie 2 - L'action.

Il est temps de passer aux choses sérieuses. C'est ici que nous rentrons un peu plus dans le détail de l'architecture d'un RAG.
La question initiale est : Lorsqu'un utilisateur formule un prompt, comment sélectionner les plus sources pertinentes?
D'ailleurs, sommes-nous obligé de sélectionner ? 
La réponse est oui. Chaque modèle est capable de travailler avec des contextes plus ou moins riches : c'est le nombre de tokens.
De manière simplifiée, on peut considérer que 1 token = 1 mot.
[Understanding Token Counts Graph]
[Lien vers l'article Medium de CriticalMynd]

Pour notre cas, le Llama3:8B que je fais tourner est capable de gérer 8000 tokens, ce qui représente 
environ 10 pages d'articles scientifiques. Problème : j'ai des articles de bien plus que 10 pages dans mon corpus. 
Et j'ai un livre de 750 pages. Devrais-je changer de PC et faire tourner un modèle plus puissant ?

En fait, ce n'est pas vraiment la peine : on va pouvoir astucieusement préparer nos données en amont, afin qu'elles 
puissent être exploitées lors de la génération de la réponse. Il n'existe pas de science exacte à  ce stade mais
un exemple de solution peut ressembler à ceci :
Pour chaque document .pdf:
1. Extraire le texte.
2. Le nettoyer, récupérer les données des graphes (Le détecteur à tâche non triviale s'active ici :) )
3. Formatter le résultat en Markdown structuré, qui est un format idéal lorsque l'on dialogue avec des LLMs.
4. Découpage des fichiers .md en chunks de longueur 1000, avec un certain overlap (afin de ne pas perdre de contexte si on coupe une phrase en deux). Ces valeurs sont ce qu'on appelle des hyper-paramètres du RAG.
  * Note: Chaque chunk hérite des métadonnées du document dont il provient: cela nous permettra de retrouver le papier source d'une information.
5. étape mystère 1 qui va être abordée par la suite.
6. étape mystère 2 qui va, aussi, être abordée par la suite.

Le "slicing" en chunk va permettre au système de digérer un nombre beaucoup plus grand d'information. 
Comme en cuisine lorsqu'on découpe une pomme de terre en cubes avant de la cuire.
Pour la suite nous travaillons donc avec des chunks, des dés de document.

Afin d'évaluer la pertinence de chaque chunk par rapport à la question posée par l'utilisateur, un score de similarité
est calculé. Ce calcul de score fait apparaître un nouvel élément dans l'architecture RAG : L'embedding.

L'embedding c'est le fait d'encoder des données afin de les positionner dans un espace à N dimensions : Comme si on donnait
à chaque chunk des coordonnées GPS. Il suffit ensuite de calculer les coordonnées GPS de la question de l'utilisateur 
pour être capable d'évaluer la distance géographique entre les deux : plus elle est proche, plus il y a de similarité
entre le chunk (et donc le document dont il provient) et le prompt utilisateur.

Mon explication cache de la complexité (passionnante) mais la logique est bien celle-ci.

Cela dit, le processus d'embedding est coûteux et ne se fait donc pas à la volée, d'autant plus que les coordonnées
GPS de nos chunks ne changent pas d'un embedding à l'autre. Donc il nous faut à ce stade un moyen pour enregistrer localement
l'information : c'est là que ChromaDB entre en jeu [expliquer ce qu'est ChromeDB et l'intérêt]

Voici sans surpise les étapes mystères qui se cachaient derrière les étapes 5 & 6 :
5. Embedding de chaque chunk en utilisant un Encodeur (qui d'ailleurs est fait par un modèle ML) 
6. Enregistrement des "Coordonnées GPS", qui sont en réalité des vecteurs, dans ChromaDB, avec indexation pour accélerer l'accès en lecture de ces informations.

* la boucle d'ingestion doit être rejouée à chaque modification du corpus


