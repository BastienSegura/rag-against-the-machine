# [Title]

## Introduction

Dans notre vie d'ingénieurs, l'usage des LLMs est devenu de plus en plus incontournable.
Que ce soit de l'aide rédactionnelle, pour faire de la rétro-ingénierie, ou même du développement, c'est un outil 
game-changer dont l'adoption dans la vie quotidienne ne fait plus aucun doute. [parler du fait que cela nous
augmente au même titre que des précédentes révolutions technologiques. donner des exemples]
[Insérer graphe d'adoption de chatgpt et autres LLMs]

Cependant, cela pose un problème sur la souveraineté et de la maîtrise que nous avons sur ces outils: où sont
enregistrées les données que l'on envoie ? A quelles fins sont-elles utilisées? Il ne fait aucun doute pour moi
que pour beaucoup d'utilisateurs de ChatGpt, l'application les connait souvent mieux que les gens qui les entoure.
Et même au delà de ces considérations, nous créons une dépendance, par notre usage quotidien de ces outils, aux plateformes
qui gèrent ces outils. Qu'est-ce qui les empêchera à l'avenir, de multiplier leur prix par 2? Par 3? Ce sont
des phénomènes loins de la fiction, qui arrivent d'ailleurs dans l'industrie du Cloud. Nombre de grands comptes,
ayant migré vers des Cloud Provider (Azure, GCP, AWS...) se retrouvent écrasés par des tarifs sur lesquels ils
perdent peu à peu la main.

Quelles conclusions en tirer? De mon point de vue, il ne faut pas non plus tomber dans la diabolisation de ces outils
et complètement les rejeter. Ils restent de formidables atout quand ils sont bien utilisés. Mais c'est aussi
une opportunité selon moi d'ouvrir le capot de ces services, comprendre comment ils fonctionnent pour mieux les
utiliser, et surtout être capable de s'en défaire si besoin. [Faire une comparaison sur une machine qu'on
utilise tous les jours, le lave-vaisselle par exemple?]

C'est pour ces raisons que j'ai souhaité me lancer un défi : serait-il possible, en 48h, de déployer en local une solution
fiable en local ?

(TL;DR : Oui c'était possible. Mais il y a un mais...)

Suivez-moi tout au long de cette expérimentation de 48H.

## Chapitre 0. La veille du jour 1

Je sais d'expérience qu'un élément critique de la réussite de ce prototype dépendra pour beaucoup de la 
qualité de son cadrage : avant même de débuter, il faut que je sache où je vais, et surtout, où je ne vais pas.
Je veux aussi en profiter pour mettre en pratique des idées d'un workflow autour des LLMs qui me permettrait
de produire plus vite et mieux, et surtout : je ne veux pas de boîte noire. Je veux apprendre et comprendre tout.

Je décide donc d'écrire un manifeste qui contiendra tout le plan de mon projet : la liste des différentes étapes,
une estimation de charge pour chacune d'entres-elle, et surtout à chaque étape : quels frameworks et librairies utiliser.

Ce manifeste sera un document clef pour piloter le projet. Il m'empêchera de dériver (ce qui n'est pas aisé quand on
découvre de nouvelles techniques, on veut tout savoir, tout de suite, trop vite.) et sera d'une grande aide lorsque, à
plusieurs reprises, je ferais appel à un LLM pour des tâches précises. Cela garantira que je n'oublierais rien et me 
fera gagner du temps, et de la pertinence.

Lors de ces recherches nocturnes, tout semble converger vers un concept. Le RAG. "Retrieval Augmented Generation".
C'est un système qui permet à un LLM de ne pas se contenter de générer une réponse en se basant uniquement sur ses connaissances
de base, mais de s'appuyer sur des sources externes, non connues lors de l'entraînement. 
Cela permet au modèle d'être plus pertinent. 
* des informations fraîches. Pas besoin de réentraîner le modèle chaque jour sur les dernières données
* Réduire les hallucinations. Le corpus fait office de "source de vérité" ce qui réduit énormément les approximations.
[Insérer un graphe minimaliste d'un RAG (LLM qui pioche dans des sources)]
C'est d'ailleurs la technique utilisée par ChatGPT, Gemini, Claude... Pour récupérer des informations sur internet. Cette
technique est assez récente car elle est née dans le milieu de la recherche en 2020 et a été progressivement adoptée 
par l'industrie à partir de 2023.

C'était donc décidé: Il fallait que je mette en place ce système. Je complète le manifeste du projet, dont l'objectif
devient le suivant:

48 heures pour développer un système RAG capable de consulter des sources scientifiques avant de répondre à un prompt.
Avec une interface graphique minimaliste.

## Jour 1 - Partie 1 - Préparatifs

La première étape consiste à se créer un corpus, une bibliographie. Dans mon cas, je décide de récupérer des documents
"fondateurs" dans le monde du Machine Learning.
[mettre les 10 documents]
Ces documents représentent 233 pages d'articles scientifiques et 750 pages pour le livre d'Hastie. Cela constituera un
premier corpus suffisant pour faire des tests. J'écris également un fichier au format bibtex afin de renseigner les métadonnées de chaque document. Il nous servira plus tard.
[https://en.wikipedia.org/wiki/BibTeX#Database_files]


Ensuite, plus qu'à se retrousser les manches et commencer.

J'installe Ollama, outil par lequel il est possible de télécharger des llms locaux et de faire tourner un serveur
qui leur transmettra nos demandes de chat. Je prends le modèle Llama3, la version avec 8 milliards de paramètres.
Je le lance, je lui pose une question. Mon GPU s'agite, et, après quelques secondes : ma réponse est là! Mais
pour le moment, rien de bien impressionnant.

Afin de tester l'api Python, j'écris un script minimaliste. Jusque là pas d'embûches...

[Montrer un screen avec le script et le résultat]

  ## Jour 1 - Partie 2 - L'architecture RAG.

  Il est temps de passer aux choses sérieuses. C'est ici que nous rentrons un peu plus dans le détail de l'architecture d'un RAG.
  La question initiale est : Lorsqu'un utilisateur formule un prompt, comment sélectionner les plus sources pertinentes?
  D'ailleurs, sommes-nous obligé de sélectionner ? 
  La réponse est oui. Chaque modèle est capable de travailler avec des contextes plus ou moins riches : c'est le nombre de tokens.
  De manière simplifiée, on peut considérer que 1 token = 1 mot.
  [Understanding Token Counts Graph]
  [Lien vers l'article Medium de CriticalMynd]

  Pour notre cas, le Llama3:8B que je fais tourner est capable de gérer 8000 tokens, ce qui représente 
  environ 10 pages d'articles scientifiques. Problème : j'ai des articles de bien plus que 10 pages dans mon corpus. 
  Et j'ai un livre de 750 pages. Devrais-je changer de PC et faire tourner un modèle plus puissant ?

  En fait, ce n'est pas vraiment la peine : on va pouvoir astucieusement préparer nos données en amont, afin qu'elles 
  puissent être exploitées lors de la génération de la réponse. Il n'existe pas de science exacte à  ce stade mais
  un exemple de solution peut ressembler à ceci :
  Pour chaque document .pdf:
  1. Extraire le texte.
  2. Le nettoyer, récupérer les données des graphes (Le détecteur à tâche non triviale s'active ici :) )
  3. Formatter le résultat en Markdown structuré, qui est un format idéal lorsque l'on dialogue avec des LLMs.
  4. Découpage des fichiers .md en chunks de longueur 1000, avec un certain overlap (afin de ne pas perdre de contexte si on coupe une phrase en deux). Ces valeurs sont ce qu'on appelle des hyper-paramètres du RAG.
    * Note: Chaque chunk hérite des métadonnées du document dont il provient: cela nous permettra de retrouver le papier source d'une information.
  5. étape mystère 1 qui va être abordée par la suite.
  6. étape mystère 2 qui va, aussi, être abordée par la suite.

  Le "slicing" en chunk va permettre au système de digérer un nombre beaucoup plus grand d'information. 
  Comme en cuisine lorsqu'on découpe une pomme de terre en cubes avant de la cuire.
  Pour la suite nous travaillons donc avec des chunks, des dés de document.

  Afin d'évaluer la pertinence de chaque chunk par rapport à la question posée par l'utilisateur, un score de similarité
  est calculé. Ce calcul de score fait apparaître un nouvel élément dans l'architecture RAG : L'embedding.

  L'embedding c'est le fait d'encoder des données afin de les positionner dans un espace à N dimensions : Comme si on donnait
  à chaque chunk des coordonnées GPS. Il suffit ensuite de calculer les coordonnées GPS de la question de l'utilisateur 
  pour être capable d'évaluer la distance géographique entre les deux : plus elle est proche, plus il y a de similarité
  entre le chunk (et donc le document dont il provient) et le prompt utilisateur.

  Mon explication cache de la complexité (passionnante) mais la logique est bien celle-ci.

  Cela dit, le processus d'embedding est coûteux et ne se fait donc pas à la volée, d'autant plus que les coordonnées
  GPS de nos chunks ne changent pas d'un embedding à l'autre. Donc il nous faut à ce stade un moyen pour enregistrer localement
  l'information : c'est là que ChromaDB entre en jeu. C'est une base de donnée vectorielle optimisée pour stocker et interroger des embeddings. Elle permet par exemple de rechercher des éléments par similarité sémentique au lieu d'avoir des
  critères précis comme l'on pourrait avoir avec un SGBD SQL classique. Cela nous permet de dire "renvoie moi les k voisins
  les plus proches de ce vecteur que je te donne".

  Voici sans surpise les étapes mystères qui se cachaient derrière les étapes 5 & 6 :
  5. Embedding de chaque chunk en utilisant un Encodeur (qui d'ailleurs est fait par un modèle ML) 
  6. Enregistrement des "Coordonnées GPS", qui sont en réalité des vecteurs, dans ChromaDB, avec indexation pour accélerer l'accès en lecture de ces informations.

  * la boucle d'ingestion doit être rejouée à chaque modification du corpus

  Voici un diagramme qui reprend tous les éléments dont j'ai parlé.

  [Insérer le diagramme Excalidraw]

Je l'ai simplifié au maximum mais il reprend les étapes fondamentales du système que je vais devoir mettre en place.


## Jour 1 - Partie 3 - L'implémentation.

A ce stade, plusieurs heures s'était déjà écoulées, et aucune ligne de code n'avait encore été écrite. Il fallait remédier
à ça, le temps jouant en ma défaveur.

Je décide de découper le projet en modules qui chacun sera responsable d'une partie du système. Nous aurons donc :

* **loader.py** le script responsable du chargement des documents et de leur traduction en texte. Il est également responsable d'aller lire le fichier de bibliographie (dont je n'ai pas parlé )
* **indexer.py** le script responsable de la partie Embedding. De la création des chunks à leur transformation en vecteur puis stockage dans chromadb.
* **engine.py** l'interface entre l'utilisateur et notre base de connaissance. C'est ici que le calcul de similarité se fait, ainsi que la génération de la réponse à l'utilisateur en se basant sur les sources les plus pertinentes.

Tout commence donc par la porte d'entrée : **`loader.py`**.

Sur le papier, charger des documents semble trivial. Une bibliothèque comme `LlamaIndex` propose d'ailleurs des outils "clés en main" très puissants comme le `SimpleDirectoryReader` qui avale des dossiers entiers de PDFs sans broncher. C'est ce que j'ai utilisé, mais je me suis vite heurté à une réalité de la Data Science : **Garbage In, Garbage Out**.

Si je donne à mon RAG une bouillie de texte brut issue d'un PDF, il pourra peut-être répondre à mes questions, mais il sera incapable de citer ses sources correctement. Pour une machine, un PDF n'est qu'une suite de caractères anonymes. Or, mon objectif est la rigueur bibliographique.

J'ai donc dû sophistiquer cette étape d'ingestion. J'ai codé une logique d'**enrichissement de métadonnées**.
Le principe est le suivant : le script ne se contente pas d'extraire le texte. Il charge en parallèle mon fichier de bibliographie (`.bib`), qui contient les informations structurées (Auteur, Titre, Année).

Concrètement, voici la mécanique que j'ai implémentée :

1. Le script scanne le dossier de PDFs.
2. Pour chaque fichier (par exemple `lecun-bottou-1998.pdf`), il isole le nom du fichier.
3. Il utilise ce nom comme une clé pour aller chercher les infos dans le fichier `.bib`.
4. Il **injecte** ces informations (Titre : "Deep Learning", Auteur : "LeCun, Bengio", Année : "2015") directement dans l'objet `Document` avant même qu'il ne soit transformé en vecteurs.

C'est une étape invisible pour l'utilisateur final, mais c'est précisément ce qui permettra plus tard au modèle de dire *"Selon LeCun (2015)..."* plutôt que de lancer une affirmation sans preuve.

### Bilan du Jour 1 : La fondation invisible

La première journée s'achève. Si je regarde mon éditeur de code, je n'ai finalement qu'un seul fichier `loader.py`. Cela peut sembler maigre, mais c'est la partie immergée de l'iceberg.
Cette journée a surtout été consacrée à l'architecture :

1. **L'environnement :** Installer Ollama, configurer l'environnement Python, s'assurer que mon GPU est bien reconnu (le nerf de la guerre en local).
2. **La Donnée :** Constituer une bibliographie `.bib` propre et télécharger les PDF correspondants.
3. **L'ingestion :** Coder ce fameux loader capable de lier les deux.

J'ai des documents enrichis. Il faut maintenant les rendre "intelligents". Fin du Jour 1.

---

## Jour 2 - indexer.py : Transformer les mots en mathématiques

Mardi matin. J'ai des objets `Document` remplis de texte, mais mon LLM (le modèle de langage) ne peut pas "lire" 50 PDFs à la volée à chaque question. Il n'a pas la mémoire pour ça, et ce serait horriblement lent.

C'est là qu'intervient **`indexer.py`**. Sa mission est double : découper et vectoriser.

### 1. Le découpage (Chunking)

Un livre ne se mange pas en une bouchée. Pour que le RAG fonctionne, il faut découper le texte en petits morceaux (chunks). J'ai configuré une taille de chunk standard, mais avec un "overlap" (recouvrement) pour ne pas couper une phrase importante au milieu.

### 2. L'Embedding (La vectorisation)

C'est le cœur de la magie. J'utilise un modèle d'embedding (ici via `HuggingFaceEmbedding` pour rester en local) qui transforme chaque morceau de texte en une liste de nombres (un vecteur).
L'idée est simple : deux phrases qui parlent de la même chose auront des vecteurs mathématiquement proches, même si elles n'utilisent pas les mêmes mots.

### 3. Le stockage (ChromaDB)

Calculer ces vecteurs prend du temps (plusieurs minutes pour ma cinquantaine de papiers). Pas question de le refaire à chaque lancement du script.
Dans mon code, j'ai implémenté une logique de persistance avec **ChromaDB**.

* **Au démarrage :** Le script vérifie si un index existe déjà sur le disque (`load_index_from_storage`).
* **Si oui :** Il le charge instantanément (0 seconde).
* **Si non :** Il lance le calcul (l'embedding) et sauvegarde le résultat.

C'est une optimisation indispensable pour l'expérience utilisateur (et celle du développeur !).

## Jour 2 - engine.py : Le cerveau et le gendarme

L'après-midi du Jour 2 est consacré à l'assemblage final : **`engine.py`**.
C'est ce fichier qui fait le pont entre la question de l'utilisateur, notre base de données vectorielle (Chroma) et le modèle de langage (Ollama).

Mais je ne voulais pas juste connecter des tuyaux. Je voulais un résultat précis. C'est ici que le **Prompt Engineering** (l'art de parler à la machine) entre en jeu.

### Le System Prompt : L'anti-baratin

Les modèles de langage sont des "beaux parleurs". Si on ne les contraint pas, ils inventent. J'ai donc défini un `SYSTEM_PROMPT` très strict dans le code. C'est une consigne invisible envoyée au modèle avant chaque question.

Je lui impose trois règles d'or :

1. **L'honnêteté intellectuelle :** *"Si l'information n'est pas dans les documents, réponds : Je ne trouve pas d'information..."*. Je préfère un bot qui admet son ignorance qu'un bot qui hallucine.
2. **La citation obligatoire :** Il doit sourcer chaque affirmation (ex: `[LeCun, 2015]`). Grâce à mon travail sur le `loader.py` la veille, il a toutes les métadonnées pour le faire.
3. **Le nettoyage LaTeX :** En lisant des papiers scientifiques, les formules mathématiques sont souvent mal extraites par les PDF (ex: `/Theta` au lieu de `$\Theta$`). J'ai instruit le modèle pour qu'il agisse comme un correcteur et reformate le LaTeX à la volée.

### Le Moteur de Requête

Enfin, la fonction `query()` orchestre le ballet :

1. Elle prend la question de l'utilisateur.
2. Elle cherche les 5 morceaux de textes les plus proches dans ChromaDB.
3. Elle envoie ces 5 morceaux + la question + le System Prompt à Ollama.
4. Elle récupère la réponse générée ET les sources utilisées.

À la fin de cette deuxième journée, j'ai un script python qui tourne dans mon terminal. Je lui pose une question sur un théorème obscur, et il me répond en citant le bon papier. Ça fonctionne. C'est moche (c'est du texte blanc sur fond noir), mais c'est vivant.

Demain, il faudra rendre ça utilisable par un humain normal : place à l'interface graphique.

## Fin du jour 2 - app.py : L'interface, ou l'art de la "paresse" intelligente

Mercredi matin. Le moteur ronronne, mais pour l'instant, il ne s'adresse qu'aux développeurs capables de lire du JSON dans un terminal noir. Pour que ce projet soit une réussite ("Product-First", comme stipulé dans mon manifeste), il faut une interface graphique (UI).

Je ne suis pas front-end developer. Centrer des `div` en CSS n'est pas mon combat. J'ai donc décidé d'appliquer à la lettre la philosophie de mon projet : utiliser l'IA pour m'augmenter.

### Le Manifeste comme Super-Prompt

Au lieu de demander bêtement à un LLM *"Fais-moi une interface Streamlit pour mon script"*, j'ai utilisé une technique de contexte avancé. J'ai fourni au modèle :

1. Le code de mon `engine.py` (pour qu'il comprenne les entrées/sorties).
2. Et surtout : **mon fichier `manifest.md**`.

Pourquoi ? Parce que le manifeste contenait déjà toutes les spécifications fonctionnelles et "l'esprit" du produit. Il décrivait l'exigence de transparence, la nécessité d'afficher les sources, et le ton "commando".

Résultat ? Le LLM n'a pas juste généré du code, il a généré **mon** application. En une itération, il a compris que :

* L'affichage des sources était critique : il a créé de lui-même une **Sidebar latérale** dédiée (`st.sidebar`) pour ne pas polluer le chat, affichant métadonnées et scores de pertinence.
* L'historique était nécessaire : il a initialisé les `st.session_state` pour garder le fil de la conversation.
* La gestion d'erreur était vitale : il a wrappé les appels dans des blocs `try/except` pour éviter que l'app ne crashe si le moteur échoue.

C'est là que réside la leçon du Jour 3 : **Mieux le contexte est défini (via le Manifeste), moins on écrit de code.** En moins de deux heures, j'avais un fichier `app.py` fonctionnel, propre, et interconnecté avec mon moteur.

L'application est lancée. Je tape une question dans la barre de chat. L'IA réfléchit, scanne mes 10 PDFs, et répond en affichant les preuves sur la droite.

C'est le moment de vérité.


## Jour 2 - Le Crash Test : RAG vs Llama3 "Nu"

Maintenant que tout est en place, il reste la question fondamentale : est-ce que toute cette architecture sert vraiment à quelque chose ? Est-ce que mon RAG local fait mieux que ChatGPT ou Llama3 utilisé "nature" ?

Pour le savoir, j'ai préparé quelques tests comparatifs sur des questions pièges issues de mes documents techniques.

*(Cette section en cours de rédaction... À suivre !)*
